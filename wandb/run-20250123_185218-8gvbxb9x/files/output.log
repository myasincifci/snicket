/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.
  warnings.warn(msg)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name      | Type              | Params
------------------------------------------------
0 | model     | ResNet            | 11.2 M
1 | train_acc | BinaryAccuracy    | 0
2 | val_acc   | BinaryAccuracy    | 0
3 | loss_fn   | BCEWithLogitsLoss | 0
------------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.708    Total estimated model params size (MB)
Sanity Checking DataLoader 0:   0%|                                                                                                                          | 0/2 [00:00<?, ?it/s]
/home/yasin/miniforge3/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)









Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████| 55/55 [00:18<00:00,  3.04it/s, v_num=xb9x, train/loss_step=0.561, train/acc_step=0.738]









Epoch 1: 100%|█| 55/55 [00:18<00:00,  3.04it/s, v_num=xb9x, train/loss_step=0.454, train/acc_step=0.785, val/loss=0.616, val/acc=0.667, train/loss_epoch=0.585, train/acc_epoch=0.7










Epoch 2: 100%|█| 55/55 [00:18<00:00,  3.02it/s, v_num=xb9x, train/loss_step=0.484, train/acc_step=0.776, val/loss=0.606, val/acc=0.680, train/loss_epoch=0.517, train/acc_epoch=0.7









Epoch 3: 100%|█| 55/55 [00:18<00:00,  3.05it/s, v_num=xb9x, train/loss_step=0.519, train/acc_step=0.720, val/loss=0.564, val/acc=0.705, train/loss_epoch=0.489, train/acc_epoch=0.7










Epoch 4: 100%|█| 55/55 [00:18<00:00,  3.03it/s, v_num=xb9x, train/loss_step=0.414, train/acc_step=0.804, val/loss=0.552, val/acc=0.726, train/loss_epoch=0.475, train/acc_epoch=0.7









Epoch 5: 100%|█| 55/55 [00:18<00:00,  3.05it/s, v_num=xb9x, train/loss_step=0.462, train/acc_step=0.785, val/loss=0.812, val/acc=0.562, train/loss_epoch=0.472, train/acc_epoch=0.7









Epoch 6: 100%|█| 55/55 [00:18<00:00,  3.04it/s, v_num=xb9x, train/loss_step=0.436, train/acc_step=0.757, val/loss=0.960, val/acc=0.600, train/loss_epoch=0.456, train/acc_epoch=0.7










Epoch 7: 100%|█| 55/55 [00:18<00:00,  3.05it/s, v_num=xb9x, train/loss_step=0.439, train/acc_step=0.785, val/loss=0.774, val/acc=0.619, train/loss_epoch=0.443, train/acc_epoch=0.7





