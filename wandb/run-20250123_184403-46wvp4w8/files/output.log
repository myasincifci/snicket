/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name      | Type              | Params
------------------------------------------------
0 | model     | ResNet            | 11.2 M
1 | train_acc | BinaryAccuracy    | 0
2 | val_acc   | BinaryAccuracy    | 0
3 | loss_fn   | BCEWithLogitsLoss | 0
------------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.708    Total estimated model params size (MB)
/home/yasin/miniforge3/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass








Epoch 0: 100%|████████████████████████████████████████████████████████████████████████████| 55/55 [00:17<00:00,  3.07it/s, v_num=p4w8, train/loss_step=0.174, train/acc_step=0.944]









Epoch 1: 100%|█| 55/55 [00:17<00:00,  3.06it/s, v_num=p4w8, train/loss_step=0.114, train/acc_step=0.953, val/loss=0.712, val/acc=0.730, train/loss_epoch=0.317, train/acc_epoch=0.8









Epoch 2: 100%|█| 55/55 [00:17<00:00,  3.06it/s, v_num=p4w8, train/loss_step=0.0703, train/acc_step=0.972, val/loss=0.360, val/acc=0.853, train/loss_epoch=0.131, train/acc_epoch=0.




















Epoch 4: 100%|█| 55/55 [00:17<00:00,  3.07it/s, v_num=p4w8, train/loss_step=0.0495, train/acc_step=0.972, val/loss=0.285, val/acc=0.909, train/loss_epoch=0.0581, train/acc_epoch=0









Epoch 5: 100%|█| 55/55 [00:18<00:00,  3.04it/s, v_num=p4w8, train/loss_step=0.041, train/acc_step=0.981, val/loss=0.315, val/acc=0.907, train/loss_epoch=0.0508, train/acc_epoch=0.




















Epoch 7: 100%|█| 55/55 [00:17<00:00,  3.07it/s, v_num=p4w8, train/loss_step=0.0276, train/acc_step=0.981, val/loss=0.542, val/acc=0.841, train/loss_epoch=0.0346, train/acc_epoch=0









Epoch 8: 100%|█| 55/55 [00:18<00:00,  3.04it/s, v_num=p4w8, train/loss_step=0.0157, train/acc_step=1.000, val/loss=0.524, val/acc=0.840, train/loss_epoch=0.0322, train/acc_epoch=0




















Epoch 10: 100%|█| 55/55 [00:18<00:00,  3.01it/s, v_num=p4w8, train/loss_step=0.00689, train/acc_step=1.000, val/loss=0.310, val/acc=0.917, train/loss_epoch=0.0273, train/acc_epoch




















Epoch 12: 100%|█| 55/55 [00:17<00:00,  3.06it/s, v_num=p4w8, train/loss_step=0.0341, train/acc_step=0.991, val/loss=0.610, val/acc=0.864, train/loss_epoch=0.0159, train/acc_epoch=





