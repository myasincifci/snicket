/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yasin/miniforge3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/yasin/miniforge3/lib/python3.10/site-packages/pytorch_lightning/loggers/wandb.py:396: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name      | Type              | Params
------------------------------------------------
0 | model     | ResNet            | 11.2 M
1 | train_acc | BinaryAccuracy    | 0
2 | val_acc   | BinaryAccuracy    | 0
3 | loss_fn   | BCEWithLogitsLoss | 0
------------------------------------------------
11.2 M    Trainable params
0         Non-trainable params
11.2 M    Total params
44.708    Total estimated model params size (MB)
Epoch 0:   0%|                                                                                                                                             | 0/220 [00:00<?, ?it/s]
/home/yasin/miniforge3/lib/python3.10/site-packages/torch/autograd/graph.py:744: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)








Epoch 0: 100%|██████████████████████████████████████████████████████████████████████████| 220/220 [00:16<00:00, 12.98it/s, v_num=1eoi, train_loss_step=0.302, train_acc_step=0.818]









Epoch 1: 100%|█| 220/220 [00:16<00:00, 12.97it/s, v_num=1eoi, train_loss_step=0.348, train_acc_step=0.818, val_loss=0.493, val_acc=0.759, train_loss_epoch=0.500, train_acc_epoch=0









Epoch 2: 100%|█| 220/220 [00:17<00:00, 12.55it/s, v_num=1eoi, train_loss_step=0.242, train_acc_step=0.909, val_loss=0.966, val_acc=0.678, train_loss_epoch=0.429, train_acc_epoch=0









Epoch 3: 100%|█| 220/220 [00:17<00:00, 12.53it/s, v_num=1eoi, train_loss_step=0.211, train_acc_step=0.909, val_loss=0.469, val_acc=0.785, train_loss_epoch=0.405, train_acc_epoch=0









Epoch 4: 100%|█| 220/220 [00:17<00:00, 12.61it/s, v_num=1eoi, train_loss_step=0.326, train_acc_step=0.818, val_loss=0.636, val_acc=0.641, train_loss_epoch=0.384, train_acc_epoch=0









Epoch 5: 100%|█| 220/220 [00:17<00:00, 12.53it/s, v_num=1eoi, train_loss_step=0.352, train_acc_step=0.818, val_loss=1.210, val_acc=0.507, train_loss_epoch=0.373, train_acc_epoch=0




